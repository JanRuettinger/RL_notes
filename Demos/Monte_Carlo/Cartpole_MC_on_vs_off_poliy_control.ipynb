{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "#from value_iteration import value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02515167044969889,\n",
       " -0.013416387699950919,\n",
       " 0.01661648358938167,\n",
       " 0.027350564042766243)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "#action = env.action_space.sample()\n",
    "#observation, reward, done, info = env.step(action)\n",
    "tuple(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 16 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 44 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 27 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Monte Carlo On-Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from MC_on_policy_control import mc_control_epsilon_greedy, sample\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7000/7000."
     ]
    }
   ],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=7000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 37 timesteps\n",
      "Episode finished after 58 timesteps\n",
      "Episode finished after 63 timesteps\n",
      "Episode finished after 97 timesteps\n",
      "Episode finished after 100 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        state = tuple(round(i,1) for i in state)\n",
    "        probabilities = policy(state)\n",
    "        totals = list(itertools.accumulate(probabilities))\n",
    "        action = sample(totals)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        if t == 99:\n",
    "            print(\"Episode finished after 100 timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Demo 3: Monte Carlo Off-Policy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from MC_off_policy_control import mc_control_importance_sampling, sample, create_random_policy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 10000/10000"
     ]
    }
   ],
   "source": [
    "random_policy = create_random_policy(env.action_space.n)\n",
    "Q, policy = mc_control_importance_sampling(env, num_episodes=10000, behavior_policy=random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 38 timesteps\n",
      "Episode finished after 40 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 47 timesteps\n",
      "Episode finished after 58 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        state = tuple(round(i,1) for i in state)\n",
    "        probabilities = policy(state)\n",
    "        totals = list(itertools.accumulate(probabilities))\n",
    "        action = sample(totals)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        if t == 99:\n",
    "            print(\"Episode finished after 100 timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
